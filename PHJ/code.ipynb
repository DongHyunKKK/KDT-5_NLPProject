{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkDF = pd.read_csv('../data/경상도.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 212906 entries, 0 to 212905\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   사투리     212906 non-null  object\n",
      " 1   표준어     212906 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "tkDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사투리</th>\n",
       "      <th>표준어</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지난달 그 지난달에는 그의 거기가 어딘지 내가 그곳을 말을 모르는데 그 정신이 들에...</td>\n",
       "      <td>지난달 그 지난달에는 그의 거기가 어딘지 내가 그곳을 말을 모르는데 그 정신이 들에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>힘들어요 농사 짓는 거는 힘들고 요즘은 기계가 좋아 가지고 농사 짓기가 수월하지만도...</td>\n",
       "      <td>힘들어요 농사 짓는 거는 힘들고 요즘은 기계가 좋아 가지고 농사 짓기가 수월하지만 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>더운 삼계탕 겉은 거는 더울 때 묵을 때는 좋지예 좋은데 묵고 나면 사람이 속도 편...</td>\n",
       "      <td>더운 삼계탕 같은 거는 더울 때 먹을 때는 좋지요 좋은데 먹고 나면 사람이 속도 편...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>장 보러 가다가 비가 마이 오길래 밭두럼 엉개질까봐 밭에 가보는 길이시더</td>\n",
       "      <td>장 보러 가다가 비가 많이 오길래 밭두렁 무너질까봐 밭에 가보는 길입니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>비가 오니까 집에 어데 누전이 되는가 그건 한번 되더라고예 그래 정전될 때는 인자 ...</td>\n",
       "      <td>비가 오니까 집에 어디 누전이 되는가 그건 한번 되더라구요 그래 정전될 때는 인제 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>우리 집에서 불편한 점은 없는데 아 그기 저기 기차역이 지나가니까 시끄러워 가지고 ...</td>\n",
       "      <td>우리 집에서 불편한 점은 없는데 아 거기 저기 기차역이 지나가니까 시끄러워 가지고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>부추는 부산 말로 정구지 아이가 정구지는 지짐을 해 먹으면은 제일 좋은데 그게 비가...</td>\n",
       "      <td>부추는 부산 말로 부추 아닌가요 부추는 전을 해 먹으면은 제일 좋은데 그게 비가 좀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>이 구두 하나만 계속 신고 다니니께 인자 굽이 딿아서 갈아야 되겠네</td>\n",
       "      <td>이 구두 하나만 계속 신고 다니니까 이제 굽이 닳아서 갈아야 되겠네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>부추 우리 말로 정구지라고 하는데 정구지는 딱 보면은 부침개 해가지고 정구지 전 구...</td>\n",
       "      <td>부추 우리 말로 부추라고 하는데 부추는 딱 보면은 부침개 해가지고 부추 전 구워가지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>요즘 몸에 좋은 거는 별 보조식품은 없는데 인제 주로 버섯 같은 거 좀 많이 먹고 ...</td>\n",
       "      <td>요즘 몸에 좋은 거는 별 보조식품은 없는데 인제 주로 버섯 같은 거 좀 많이 먹고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>예 시골 좋죠 도시도 좋고 전 시골에서 자랐습니다 많이 오랜 세월 시골에서 자랐고 ...</td>\n",
       "      <td>예 시골 좋죠 도시도 좋고 전 시골에서 자랐습니다 많이 오랜 세월 시골에서 자랐고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>오늘 비가 올 것 같으니까 우산 챙겨 가고 잊아삐지 말고 단디 챙겨가 오너래이</td>\n",
       "      <td>오늘 비가 올 것 같으니까 우산 챙겨 가고 잊어버리지 말고 꼭 챙겨서 와라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>건강을 위해서 여러 가지 좋은 운동은 못합니다마는 하나는 거의 매일 한 팔천 보 정...</td>\n",
       "      <td>건강을 위해서 여러 가지 좋은 운동은 못합니다마는 하나는 거의 매일 한 팔천 보 정...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>우리는 집에서 꼬추장 덴장을 다 담아서 먹는데 우리 따른 사람들은 우린 집에서 직접...</td>\n",
       "      <td>우리는 집에서 고추장 된장을 다 담아서 먹는데 우리 다른 사람들은 우린 집에서 직접...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>엣날에는 묵을 것이 없으니께 겨울에는 고도롬도 맛있다꼬 따 먹었지예</td>\n",
       "      <td>옛날에는 먹을 것이 없으니까 겨울에는 고드름도 맛있다고 따 먹었지요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>지는 좋아하는 반찬이 감자 찌짓는 거 하고 또 나물 무쳤는 거 하고 또 인제 된장하...</td>\n",
       "      <td>저는 좋아하는 반찬이 감자 지진 거 하고 또 나물 무쳤는 거 하고 또 이제 된장하고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>음식 묵을 음식 묵으만 계속 설사하고 퇴하고 할 때는 물을 많이 잡수시 잡숫고 병에...</td>\n",
       "      <td>음식 먹을 음식 먹으면 계속 설사하고 토하고 할 때는 물을 많이 잡수시 잡수시고 병...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>집안에 소 묵일 때 쓰는 시설이나 도굿바는 어떤 기 있습니껴</td>\n",
       "      <td>집안에 소 먹일 때 쓰는 시설이나 도구는 어떤 게 있습니까</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>내가 어릴 때는 우리 내가 한 도시에 안 살고 농사 짓는데 촌에 좀 살았기 때문에 ...</td>\n",
       "      <td>내가 어릴 때는 우리 내가 한 도시에 안 살고 농사 짓는데 촌에 좀 살았기 때문에 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>겨울 지나고 그 따시믄 봄이 왔다고 느끼고 봄에는 나물 캐러 가지 나물 캐갖고 맛있...</td>\n",
       "      <td>겨울 지나고 그 따뜻하면 봄이 왔다고 느끼고 봄에는 나물 캐러 가지 나물 캐갖고 맛...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  사투리  \\\n",
       "0   지난달 그 지난달에는 그의 거기가 어딘지 내가 그곳을 말을 모르는데 그 정신이 들에...   \n",
       "1   힘들어요 농사 짓는 거는 힘들고 요즘은 기계가 좋아 가지고 농사 짓기가 수월하지만도...   \n",
       "2   더운 삼계탕 겉은 거는 더울 때 묵을 때는 좋지예 좋은데 묵고 나면 사람이 속도 편...   \n",
       "3            장 보러 가다가 비가 마이 오길래 밭두럼 엉개질까봐 밭에 가보는 길이시더   \n",
       "4   비가 오니까 집에 어데 누전이 되는가 그건 한번 되더라고예 그래 정전될 때는 인자 ...   \n",
       "5   우리 집에서 불편한 점은 없는데 아 그기 저기 기차역이 지나가니까 시끄러워 가지고 ...   \n",
       "6   부추는 부산 말로 정구지 아이가 정구지는 지짐을 해 먹으면은 제일 좋은데 그게 비가...   \n",
       "7               이 구두 하나만 계속 신고 다니니께 인자 굽이 딿아서 갈아야 되겠네   \n",
       "8   부추 우리 말로 정구지라고 하는데 정구지는 딱 보면은 부침개 해가지고 정구지 전 구...   \n",
       "9   요즘 몸에 좋은 거는 별 보조식품은 없는데 인제 주로 버섯 같은 거 좀 많이 먹고 ...   \n",
       "10  예 시골 좋죠 도시도 좋고 전 시골에서 자랐습니다 많이 오랜 세월 시골에서 자랐고 ...   \n",
       "11        오늘 비가 올 것 같으니까 우산 챙겨 가고 잊아삐지 말고 단디 챙겨가 오너래이   \n",
       "12  건강을 위해서 여러 가지 좋은 운동은 못합니다마는 하나는 거의 매일 한 팔천 보 정...   \n",
       "13  우리는 집에서 꼬추장 덴장을 다 담아서 먹는데 우리 따른 사람들은 우린 집에서 직접...   \n",
       "14              엣날에는 묵을 것이 없으니께 겨울에는 고도롬도 맛있다꼬 따 먹었지예   \n",
       "15  지는 좋아하는 반찬이 감자 찌짓는 거 하고 또 나물 무쳤는 거 하고 또 인제 된장하...   \n",
       "16  음식 묵을 음식 묵으만 계속 설사하고 퇴하고 할 때는 물을 많이 잡수시 잡숫고 병에...   \n",
       "17                  집안에 소 묵일 때 쓰는 시설이나 도굿바는 어떤 기 있습니껴   \n",
       "18  내가 어릴 때는 우리 내가 한 도시에 안 살고 농사 짓는데 촌에 좀 살았기 때문에 ...   \n",
       "19  겨울 지나고 그 따시믄 봄이 왔다고 느끼고 봄에는 나물 캐러 가지 나물 캐갖고 맛있...   \n",
       "\n",
       "                                                  표준어  \n",
       "0   지난달 그 지난달에는 그의 거기가 어딘지 내가 그곳을 말을 모르는데 그 정신이 들에...  \n",
       "1   힘들어요 농사 짓는 거는 힘들고 요즘은 기계가 좋아 가지고 농사 짓기가 수월하지만 ...  \n",
       "2   더운 삼계탕 같은 거는 더울 때 먹을 때는 좋지요 좋은데 먹고 나면 사람이 속도 편...  \n",
       "3            장 보러 가다가 비가 많이 오길래 밭두렁 무너질까봐 밭에 가보는 길입니다  \n",
       "4   비가 오니까 집에 어디 누전이 되는가 그건 한번 되더라구요 그래 정전될 때는 인제 ...  \n",
       "5   우리 집에서 불편한 점은 없는데 아 거기 저기 기차역이 지나가니까 시끄러워 가지고 ...  \n",
       "6   부추는 부산 말로 부추 아닌가요 부추는 전을 해 먹으면은 제일 좋은데 그게 비가 좀...  \n",
       "7               이 구두 하나만 계속 신고 다니니까 이제 굽이 닳아서 갈아야 되겠네  \n",
       "8   부추 우리 말로 부추라고 하는데 부추는 딱 보면은 부침개 해가지고 부추 전 구워가지...  \n",
       "9   요즘 몸에 좋은 거는 별 보조식품은 없는데 인제 주로 버섯 같은 거 좀 많이 먹고 ...  \n",
       "10  예 시골 좋죠 도시도 좋고 전 시골에서 자랐습니다 많이 오랜 세월 시골에서 자랐고 ...  \n",
       "11          오늘 비가 올 것 같으니까 우산 챙겨 가고 잊어버리지 말고 꼭 챙겨서 와라  \n",
       "12  건강을 위해서 여러 가지 좋은 운동은 못합니다마는 하나는 거의 매일 한 팔천 보 정...  \n",
       "13  우리는 집에서 고추장 된장을 다 담아서 먹는데 우리 다른 사람들은 우린 집에서 직접...  \n",
       "14              옛날에는 먹을 것이 없으니까 겨울에는 고드름도 맛있다고 따 먹었지요  \n",
       "15  저는 좋아하는 반찬이 감자 지진 거 하고 또 나물 무쳤는 거 하고 또 이제 된장하고...  \n",
       "16  음식 먹을 음식 먹으면 계속 설사하고 토하고 할 때는 물을 많이 잡수시 잡수시고 병...  \n",
       "17                   집안에 소 먹일 때 쓰는 시설이나 도구는 어떤 게 있습니까  \n",
       "18  내가 어릴 때는 우리 내가 한 도시에 안 살고 농사 짓는데 촌에 좀 살았기 때문에 ...  \n",
       "19  겨울 지나고 그 따뜻하면 봄이 왔다고 느끼고 봄에는 나물 캐러 가지 나물 캐갖고 맛...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkDF.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkDF = pd.melt(tkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkDF = tkDF.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복값 제거\n",
    "tkDF.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['사투리', '표준어', '사투리', ..., '사투리', '표준어', '표준어'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkDF['variable'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "tkDF['value'] = tkDF['value'].replace(r'[{}]'.format(string.punctuation), '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 308433 entries, 0 to 425811\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   variable  308433 non-null  object\n",
      " 1   value     308433 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "tkDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.DF = df\n",
    "        self.label = [1 if value == '사투리' else 0 for value in self.DF['variable'].values]\n",
    "        self.text = self.DF['value'].values\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.label[idx],self.text[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkDS = dataset(tkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(0.8 * len(tkDS))\n",
    "val_length = len(tkDS) - train_length\n",
    "trainDS, valDS = random_split(tkDS, [train_length, val_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 관련 특별 문자\n",
    "unk = '<UNK>'\n",
    "pad = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 불용어 제거 # 어근 제거는 안함 - 사투리와 표준어 구별 사라짐\n",
    "def torkenizer(text):\n",
    "    with open('data/hangul_stopword.txt', 'r', encoding='utf-8') as f:\n",
    "        stopword_h = {line.strip() for line in f.readlines()} \n",
    "    token = mecab.morphs(text)\n",
    "    token = [word for word in token if word not in stopword_h] \n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield torkenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install mecab-ko-msvc mecab-ko-dic-msvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = build_vocab_from_iterator(yield_tokens(trainDS), specials=[unk, pad], special_first=True)\n",
    "\n",
    "VOCAB.set_default_index(VOCAB[unk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 > 정수 인코딩\n",
    "text_pipeline = lambda x: VOCAB(torkenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩\n",
    "token2id = {label: id for id, label in enumerate(VOCAB.get_itos())}\n",
    "# 디코딩\n",
    "id2token = {id: label for id, label in enumerate(VOCAB.get_itos())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 크기 만큼 데이터셋 반환 함수\n",
    "def collate_batch(batch):\n",
    "\n",
    "    label_list, text_list, offsets = [], [], [0] \n",
    "\n",
    "    for (_label, _text) in batch:\n",
    "\n",
    "        label_list.append(_label)\n",
    "\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    # 텐서화 진행\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # 누적값 -> 전체 갯수\n",
    "    text_list = torch.cat(text_list)\n",
    "    \n",
    "    return label_list.to(DEVICE), text_list.to(DEVICE), offsets.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "trainDL = DataLoader(trainDS, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "testDL = DataLoader(valDS, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLASSES: 1 VOCAB_SIZE: 47562\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSE = 1\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "NUM_LAYER = 1\n",
    "print(f'NUM_CLASSES: {NUM_CLASSE} VOCAB_SIZE: {VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_class):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_size, NUM_LAYER, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self,text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        return self.fc(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 3\n",
    "EMBEDDING_DIM = 64\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "EPOCHS = 10\n",
    "LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 관련 인스턴스\n",
    "MODEL = TextModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, NUM_CLASSE).to(DEVICE)\n",
    "\n",
    "CRITERION = nn.BCEWithLogitsLoss()\n",
    "OPTIMIZER = torch.optim.Adam(MODEL.parameters(), lr=LR)\n",
    "SCHEDULER = torch.optim.lr_scheduler.ReduceLROnPlateau(OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, DL, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    lossList = []\n",
    "\n",
    "    f1 = BinaryF1Score()\n",
    "\n",
    "    for label, text, offset in DL: \n",
    "        label = label.to(DEVICE)\n",
    "        text = text.to(DEVICE)  \n",
    "        offset = offset.to(DEVICE)\n",
    "        \n",
    "        output = model(text, offset)\n",
    "\n",
    "        target = label.unsqueeze(1).float()  # [배치 크기, 1] 형태로 변환\n",
    "        \n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lossList.append(loss.item())\n",
    "        \n",
    "        f1(output, target)\n",
    "\n",
    "    train_loss = np.mean(lossList)\n",
    "    train_f1 = f1.compute().cpu().item()\n",
    "\n",
    "    print(f'[Train loss] ==> {train_loss:.4f}    [Train Accuracy] ==> {train_f1:4f}')\n",
    "    return train_loss, train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, DL, loss_fn):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    f1 = BinaryF1Score()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text, offset in DL: \n",
    "            label = label.to(DEVICE)\n",
    "            text = text.to(DEVICE)\n",
    "            offset = offset.to(DEVICE)\n",
    "            \n",
    "            output = model(text, offset)\n",
    "\n",
    "            \n",
    "            target = label.unsqueeze(1).float()  # [배치 크기, 1] 형태로 변환\n",
    "            \n",
    "            loss = loss_fn(output, target)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            f1(output, target)\n",
    "        \n",
    "    val_loss = np.mean(losses)\n",
    "    val_f1 = f1.compute().cpu().item()\n",
    "\n",
    "    print(f'[Valid loss] ==> {val_loss:4f}    [Valid Accuracy] ==> {val_f1:4f}')\n",
    "    return val_loss, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        # 토큰화 > 정수 변환 > 텐서\n",
    "        text = torch.tensor(text_pipeline(text), dtype=torch.int64).to(DEVICE)\n",
    "        text = text.unsqueeze(0)\n",
    "        offsets = torch.tensor([0]).DEVICE       \n",
    "        predicted_label = model(text, offsets)\n",
    "        return predicted_label.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10]\n",
      "[Train loss] ==> 0.5072    [Train Accuracy] ==> 0.747324\n",
      "[Valid loss] ==> 0.499009    [Valid Accuracy] ==> 0.733524\n",
      "[Epoch 2/10]\n",
      "[Train loss] ==> 0.4928    [Train Accuracy] ==> 0.762998\n",
      "[Valid loss] ==> 0.490618    [Valid Accuracy] ==> 0.740987\n",
      "[Epoch 3/10]\n",
      "[Train loss] ==> 0.4807    [Train Accuracy] ==> 0.768460\n",
      "[Valid loss] ==> 0.481046    [Valid Accuracy] ==> 0.762715\n",
      "[Epoch 4/10]\n",
      "[Train loss] ==> 0.4751    [Train Accuracy] ==> 0.765316\n",
      "[Valid loss] ==> 0.489634    [Valid Accuracy] ==> 0.768920\n",
      "[Epoch 5/10]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     train_loss, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainDL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCRITERION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     val_loss, val_f1 \u001b[38;5;241m=\u001b[39m evaluate(MODEL, testDL, CRITERION)\n\u001b[0;32m     12\u001b[0m     train_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[68], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, DL, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 20\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m lossList\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     24\u001b[0m f1(output, target)\n",
      "File \u001b[1;32mc:\\Users\\mybae\\anaconda3\\envs\\TORCH17_NLP38\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mybae\\anaconda3\\envs\\TORCH17_NLP38\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\mybae\\anaconda3\\envs\\TORCH17_NLP38\\lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\mybae\\anaconda3\\envs\\TORCH17_NLP38\\lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mybae\\anaconda3\\envs\\TORCH17_NLP38\\lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    388\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 및 검증 진행\n",
    "train_ = [[],[]]\n",
    "test_ = [[],[]]\n",
    "\n",
    "min_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'[Epoch {epoch}/{EPOCHS}]')\n",
    "    train_loss, train_f1 = train(MODEL, trainDL, CRITERION, OPTIMIZER)\n",
    "    val_loss, val_f1 = evaluate(MODEL, testDL, CRITERION)\n",
    "\n",
    "    train_[0].append(train_loss)\n",
    "    train_[1].append(train_f1)\n",
    "    train_[0].append(train_loss)\n",
    "    train_[1].append(train_f1)\n",
    "\n",
    "    SCHEDULER.step(val_loss)\n",
    "\n",
    "    if min_loss > val_loss:\n",
    "        min_loss = val_loss\n",
    "        torch.save(MODEL, 'model.tkLang.pt')\n",
    "    if SCHEDULER.num_bad_epochs >= SCHEDULER.patience:\n",
    "        print(f'Early Stopping at {epoch}')\n",
    "        torch.save(MODEL, 'model.tkLang.pt')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH17_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
